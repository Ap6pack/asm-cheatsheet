# GitLab CI ASM Pipeline
# Enterprise-grade continuous attack surface monitoring for GitLab

stages:
  - discovery
  - enumeration
  - vulnerability-scan
  - analysis
  - notify

variables:
  SCAN_OUTPUT_DIR: "asm-results"
  ARTIFACT_RETENTION: "30 days"
  MAX_PARALLEL_JOBS: "5"
  DOCKER_IMAGE: "ghcr.io/asm-toolkit/scanner:latest"
  TARGET_DOMAIN: "${CI_COMMIT_REF_NAME == 'main' ? 'example.com' : 'staging.example.com'}"

# Cache configuration for ASM tools
cache:
  key: "${CI_COMMIT_REF_SLUG}-asm"
  paths:
    - .cache/nuclei-templates
    - .cache/amass
    - ${SCAN_OUTPUT_DIR}/

# Default job configuration
default:
  image: ${DOCKER_IMAGE}
  before_script:
    - mkdir -p ${SCAN_OUTPUT_DIR}
    - echo "Starting ASM scan for ${TARGET_DOMAIN}"
  retry:
    max: 2
    when:
      - runner_system_failure
      - stuck_or_timeout_failure

# Job 1: Asset Discovery
discovery:
  stage: discovery
  timeout: 1h
  script:
    - mkdir -p ${SCAN_OUTPUT_DIR}/discovery
    
    # Parallel subdomain discovery
    - |
      {
        subfinder -d "${TARGET_DOMAIN}" -all -silent
        amass enum -passive -d "${TARGET_DOMAIN}" -silent
        curl -s "https://crt.sh/?q=%.${TARGET_DOMAIN}&output=json" | jq -r ".[].name_value" || true
      } | sort -u > ${SCAN_OUTPUT_DIR}/discovery/subdomains.txt
    
    # Count discovered subdomains
    - echo "SUBDOMAIN_COUNT=$(wc -l < ${SCAN_OUTPUT_DIR}/discovery/subdomains.txt)" >> discovery.env
    
    # Compare with baseline
    - |
      if [ -f "baseline/subdomains.txt" ]; then
        NEW_ASSETS=$(comm -13 <(sort baseline/subdomains.txt) <(sort ${SCAN_OUTPUT_DIR}/discovery/subdomains.txt) | wc -l)
        if [ "$NEW_ASSETS" -gt 0 ]; then
          echo "HAS_NEW_ASSETS=true" >> discovery.env
          echo "ðŸ†• Found $NEW_ASSETS new subdomains"
        else
          echo "HAS_NEW_ASSETS=false" >> discovery.env
        fi
      else
        echo "HAS_NEW_ASSETS=true" >> discovery.env
      fi
    
    # Export results
    - cat discovery.env
    
  artifacts:
    reports:
      dotenv: discovery.env
    paths:
      - ${SCAN_OUTPUT_DIR}/discovery/
    expire_in: ${ARTIFACT_RETENTION}
  
  only:
    - main
    - develop
    - schedules
    - web

# Job 2: Service Enumeration (Parallel)
.enumeration_template:
  stage: enumeration
  needs: ["discovery"]
  timeout: 1h 30m
  script:
    - mkdir -p ${SCAN_OUTPUT_DIR}/enumeration
    
    # Split workload for parallel processing
    - |
      TOTAL_LINES=$(wc -l < ${SCAN_OUTPUT_DIR}/discovery/subdomains.txt)
      BATCH_SIZE=$((TOTAL_LINES / ${MAX_PARALLEL_JOBS} + 1))
      split -l ${BATCH_SIZE} ${SCAN_OUTPUT_DIR}/discovery/subdomains.txt batch_
      mv batch_${CI_NODE_INDEX} current_batch.txt || touch current_batch.txt
    
    # Check live hosts
    - |
      cat current_batch.txt | \
        httpx -silent -status-code -title -tech-detect -json \
        > ${SCAN_OUTPUT_DIR}/enumeration/live_hosts_${CI_NODE_INDEX}.json
    
    # Port scanning
    - |
      cat current_batch.txt | \
        dnsx -silent -a -resp | cut -d" " -f2 | \
        naabu -silent -top-ports 1000 \
        > ${SCAN_OUTPUT_DIR}/enumeration/ports_${CI_NODE_INDEX}.txt
    
  artifacts:
    paths:
      - ${SCAN_OUTPUT_DIR}/enumeration/
    expire_in: ${ARTIFACT_RETENTION}
  
  parallel: 5

enumeration:
  extends: .enumeration_template
  only:
    - main
    - develop
    - schedules
    - web

# Job 3: Vulnerability Scanning
vulnerability_scan:
  stage: vulnerability-scan
  needs: ["enumeration"]
  timeout: 2h
  image: projectdiscovery/nuclei:latest
  script:
    - mkdir -p ${SCAN_OUTPUT_DIR}/consolidated
    
    # Consolidate enumeration results
    - |
      find ${SCAN_OUTPUT_DIR}/enumeration/ -name "live_hosts_*.json" -exec cat {} \; \
        > ${SCAN_OUTPUT_DIR}/consolidated/all_live_hosts.json
      find ${SCAN_OUTPUT_DIR}/enumeration/ -name "ports_*.txt" -exec cat {} \; | sort -u \
        > ${SCAN_OUTPUT_DIR}/consolidated/all_ports.txt
    
    # Extract URLs for scanning
    - |
      jq -r '.url' ${SCAN_OUTPUT_DIR}/consolidated/all_live_hosts.json \
        > ${SCAN_OUTPUT_DIR}/consolidated/urls.txt
    
    # Run Nuclei vulnerability scan
    - |
      nuclei -l ${SCAN_OUTPUT_DIR}/consolidated/urls.txt \
        -t cves/ -t vulnerabilities/ -t exposures/ \
        -severity critical,high,medium \
        -o ${SCAN_OUTPUT_DIR}/vulnerabilities.txt \
        -json-export ${SCAN_OUTPUT_DIR}/vulnerabilities.json \
        -rate-limit 10 \
        -bulk-size 25 \
        -concurrency 10
    
    # Check for critical vulnerabilities
    - |
      if [ -f "${SCAN_OUTPUT_DIR}/vulnerabilities.json" ]; then
        CRITICAL_COUNT=$(jq '[.[] | select(.severity == "critical")] | length' ${SCAN_OUTPUT_DIR}/vulnerabilities.json)
        HIGH_COUNT=$(jq '[.[] | select(.severity == "high")] | length' ${SCAN_OUTPUT_DIR}/vulnerabilities.json)
        
        echo "CRITICAL_VULNS=${CRITICAL_COUNT}" >> vuln.env
        echo "HIGH_VULNS=${HIGH_COUNT}" >> vuln.env
        
        if [ "$CRITICAL_COUNT" -gt 0 ]; then
          echo "âŒ Found $CRITICAL_COUNT critical vulnerabilities!"
          exit 1  # Fail the pipeline for critical vulnerabilities
        fi
        
        if [ "$HIGH_COUNT" -gt 0 ]; then
          echo "âš ï¸ Found $HIGH_COUNT high severity vulnerabilities"
        fi
      fi
  
  artifacts:
    reports:
      dotenv: vuln.env
    paths:
      - ${SCAN_OUTPUT_DIR}/vulnerabilities.*
      - ${SCAN_OUTPUT_DIR}/consolidated/
    expire_in: ${ARTIFACT_RETENTION}
  
  allow_failure: true
  only:
    - main
    - develop
    - schedules
    - web

# Job 4: Security Analysis & Reporting
analysis:
  stage: analysis
  needs: 
    - job: discovery
      artifacts: true
    - job: enumeration
      artifacts: true
    - job: vulnerability_scan
      artifacts: true
  timeout: 30m
  image: ghcr.io/asm-toolkit/analyzer:latest
  script:
    # Generate comprehensive HTML report
    - |
      python3 /tools/generate_report.py \
        --input ${SCAN_OUTPUT_DIR} \
        --output asm_report.html \
        --format html \
        --include-screenshots \
        --risk-scoring
    
    # Generate JSON report for API consumption
    - |
      python3 /tools/generate_report.py \
        --input ${SCAN_OUTPUT_DIR} \
        --output asm_report.json \
        --format json
    
    # Calculate security score
    - |
      SCORE=100
      
      if [ -f "${SCAN_OUTPUT_DIR}/vulnerabilities.json" ]; then
        CRITICAL=$(jq '[.[] | select(.severity == "critical")] | length' ${SCAN_OUTPUT_DIR}/vulnerabilities.json || echo 0)
        HIGH=$(jq '[.[] | select(.severity == "high")] | length' ${SCAN_OUTPUT_DIR}/vulnerabilities.json || echo 0)
        MEDIUM=$(jq '[.[] | select(.severity == "medium")] | length' ${SCAN_OUTPUT_DIR}/vulnerabilities.json || echo 0)
        
        SCORE=$((SCORE - CRITICAL * 20 - HIGH * 10 - MEDIUM * 5))
      fi
      
      [ $SCORE -lt 0 ] && SCORE=0
      
      echo "SECURITY_SCORE=${SCORE}" >> analysis.env
      echo "ðŸ”’ Security Score: ${SCORE}/100"
    
    # Generate markdown summary
    - |
      cat > summary.md << EOF
      ## ASM Security Report
      
      **Domain:** ${TARGET_DOMAIN}
      **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
      **Security Score:** ${SCORE}/100
      
      ### Findings Summary
      - Subdomains discovered: ${SUBDOMAIN_COUNT:-0}
      - New assets found: ${HAS_NEW_ASSETS:-false}
      - Critical vulnerabilities: ${CRITICAL_VULNS:-0}
      - High vulnerabilities: ${HIGH_VULNS:-0}
      
      ### Report Files
      - [HTML Report](asm_report.html)
      - [JSON Report](asm_report.json)
      - [Vulnerability Details](${SCAN_OUTPUT_DIR}/vulnerabilities.json)
      EOF
  
  artifacts:
    reports:
      dotenv: analysis.env
    paths:
      - asm_report.*
      - summary.md
      - ${SCAN_OUTPUT_DIR}/
    expire_in: ${ARTIFACT_RETENTION}
  
  when: always
  only:
    - main
    - develop
    - schedules
    - web

# Job 5: Notifications
notify:
  stage: notify
  needs: 
    - job: analysis
      artifacts: true
  image: alpine:latest
  before_script:
    - apk add --no-cache curl jq
  script:
    # Slack notification
    - |
      if [ -n "${SLACK_WEBHOOK}" ]; then
        SCORE="${SECURITY_SCORE:-0}"
        COLOR="good"
        [ $SCORE -lt 80 ] && COLOR="warning"
        [ $SCORE -lt 60 ] && COLOR="danger"
        
        curl -X POST ${SLACK_WEBHOOK} \
          -H 'Content-Type: application/json' \
          -d "{
            \"attachments\": [{
              \"color\": \"${COLOR}\",
              \"title\": \"ASM Security Scan Complete\",
              \"text\": \"Security Score: ${SCORE}/100\",
              \"fields\": [
                {
                  \"title\": \"Domain\",
                  \"value\": \"${TARGET_DOMAIN}\",
                  \"short\": true
                },
                {
                  \"title\": \"Pipeline\",
                  \"value\": \"${CI_PIPELINE_URL}\",
                  \"short\": true
                }
              ],
              \"footer\": \"GitLab ASM Pipeline\",
              \"ts\": $(date +%s)
            }]
          }"
      fi
    
    # Email notification for critical findings
    - |
      if [ "${CRITICAL_VULNS:-0}" -gt 0 ]; then
        echo "Critical vulnerabilities found! Sending alert email..."
        # GitLab will send email notifications based on pipeline status
      fi
    
    # Update metrics dashboard
    - |
      if [ -n "${METRICS_API_KEY}" ]; then
        curl -X POST https://metrics.example.com/api/v1/metrics \
          -H "Authorization: Bearer ${METRICS_API_KEY}" \
          -H "Content-Type: application/json" \
          -d "{
            \"metric\": \"asm.security_score\",
            \"value\": ${SECURITY_SCORE:-0},
            \"tags\": {
              \"domain\": \"${TARGET_DOMAIN}\",
              \"pipeline_id\": \"${CI_PIPELINE_ID}\"
            }
          }"
      fi
  
  when: always
  only:
    - main
    - develop
    - schedules
    - web

# Scheduled scan configuration
scheduled_scan:
  extends: discovery
  only:
    - schedules
  variables:
    TARGET_DOMAIN: "${SCHEDULED_DOMAIN}"
    SCAN_TYPE: "full"

# Manual trigger for specific domain
manual_scan:
  extends: discovery
  when: manual
  only:
    - web
  variables:
    TARGET_DOMAIN: "${MANUAL_DOMAIN}"
